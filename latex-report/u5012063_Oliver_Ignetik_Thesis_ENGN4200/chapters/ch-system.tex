\SetPicSubDir{ch-System}

\chapter{System Design}
\label{ch:system}
\vspace{2em}

The \ac{AMT} problem can be divided into several subtasks, which include: multipitch
detection, note onset/offset detection, loudness estimation and quantisation,
instrument recognition, extraction of rhythmic information, and time
quantisation. The core problem in automatic transcription is the estimation of
concurrent pitches in a time frame, also called multiple-F0 or multi-pitch
detection. \cite{amtfc2013:Benetos}

\begin{figure}
    \centering
    \includegraphics[width=.7\columnwidth]{\Pic{png}{model}}
    \vspace{\BeforeCaptionVSpace}
    \caption[AMT architecture]{Typical architecture of an \ac{AMT} system. \cite{amtfc2013:Benetos}}
    \label{system:fig:basic-model}
\end{figure}

\section{Preliminaries}

This report will investigate how to use \ac{NMF} in interpreting an audio recording and extracting
music information from this recording. There are two systems and two applications that will be
be presented in this section.

\begin{enumerate}
    \item \ac{NMF} applied to single line melody - exploration of parameters used to fine tune model accuracy
          \begin{enumerate}
              \item MAPS Dataset - chromatic scale played on a Bechstein D 280 in a concert hall \cite{MAPS:Emiya}
              \item Type of architecture - Supervised \ac{NMF}
          \end{enumerate}
    \item \ac{NN} applied to a large music database
          \begin{enumerate}
              \item Dataset MusicNet recordings and active frame note labels \cite{thickstun2018invariances}
              \item Type of architecture - Feed Forward neural network with mutli-label classification
          \end{enumerate}
\end{enumerate}

\section{Methodology}

\subsection{NMF application to monophonic AMT}

The sample used in this analysis is a recording of a chromatic scale played on
a Bechstein D 280 piano in a concert hall environment retrieved from the MAPS database \cite{MAPS:Emiya}.

\begin{table}
    \centering
    \begin{tabular}{|l|l|l|}
        \hline
        Onset Time  (s) & Offset Time (s) & MIDI pitch \\ \hline
        1.12            & 1.42            & 23         \\ \hline
    \end{tabular}
    \caption{Example ground truths format}
    \label{system:table:nmf-ground-truths}
\end{table}

It should be noted that in this experiment the offset time will be omitted from
the investigation as this is much more difficult to infer from the audio signal.
The audio data is loaded into the python kernel using LibROSA and then it is
passed to the \ac{NMF}\_model class which is the main class for performing the
experiments in this investigation. As can be seen in
\autoref{system:fig:nmf-model} the audio data is passed to the \ac{NMF} class where
the constructor takes two important parameters to determine the spectrogram
\emph{hop length} and \emph{window size}. The \ac{NMF} class has the following
important methods :

\begin{itemize}
    \item make\_stft - builds the spectrogram based on the hop length and window size
    \item \ac{NMF}\_decomposition - performs the \ac{NMF} decomposition
    \item estimate\_onsets - find the note onsets using the peak picking algorithm
    \item estimate\_pitches - find the active pitches in the sample
\end{itemize}


\begin{figure}
    \centering
    \includegraphics[width=.95\columnwidth]{\Pic{png}{NMF-system}}
    \vspace{\BeforeCaptionVSpace}
    \caption{System model for NMF AMT}
    \label{system:fig:nmf-model}
\end{figure}

\subsubsection{Evaluation}

A set of parameters to test in the model is instantiated as a dictionary with
name and data keys to allow for flexible access. This dictionary is then
iterated through and the models accuracy for each parameter is evaluated using
the mir\_eval library.

\subsection{NN Application to polyphonic AMT}
A feed forward \ac{NN} will be used to transcribe music from the musicnet database.
The pieces are polyphonic which means the complexity is much higher due to
multiple instruments being played at the same time.

\begin{figure}[!b]
    \centering
    \includegraphics[width=.95\columnwidth]{\Pic{png}{NN-system}}
    \vspace{\BeforeCaptionVSpace}
    \caption{System model for NN AMT}
    \label{system:fig:nn-model}
\end{figure}

A baseline model is established which will be tuned by scanning the
hyperparameters of the model. Once optimal parameters have been established the
model will be trained for a larger number of epochs to learn the dataset more
comprehensively. The model will be evaluated using an appropriate metric for
multilabel classification tasks. The model will be implemented using Keras with a
Tensorflow backend and a high level overview of the model is shown in \autoref{system:fig:nn-model}
The defining features of the model architecture are provided in \autoref{system:table:baseline-parameters}

\begin{table}
    \centering
    \begin{tabular}{|l|c|}
        \hline
        \multicolumn{1}{|c|}{\textbf{Parameters}} & \textbf{Value}   \\ \hline
        Input nodes                               & 252              \\ \hline
        Output nodes                              & 88               \\ \hline
        Optimizer                                 & Adam             \\ \hline
        Output activation                         & Sigmoid function \\ \hline
        Hidden activation                         & ReLU function    \\ \hline
    \end{tabular}
    \caption{Critical Baseline model parameters}
    \label{system:table:baseline-parameters}
\end{table}

\subsection{Dataset}

All of the data for the model comes from the musicnet database \cite{thickstun2018invariances}.
The outstanding feature of this dataset is that each audio signal comes with the associated ground truth
onset and offset times and pitches. The data is split into training, validation and test sets based on a 60-20-20
percentage ratio.

\subsubsection{Features}
The audio signal is transformed into the \ac{CQT} domain
which is closely related to the STFT but is better for multipitch scenarios due
to the higher fidelity in note resolution. The \ac{CQT} is downsampled from
44.1~$\si{\kilo\hertz}$ to 16~$\si{\kilo\hertz}$ with a hop length of 512
corresponding to 32 $\si{\milli\second}$ frames. Consequently there are 252 \ac{CQT}
features per frame. Therefore, a data matrix with the size of $252 \times
    \text{number of frames}$ is obtained from the \ac{CQT} transformation.

\subsubsection{Labels}
The ground truth labels are sampled at each frame to determine the active notes
in the audio. There are 88 possible notes on the piano and as such an array with
length 88 constitues the active and inactive notes at each frame in the audio,
with 1 representing active and 0 representing inactive.

\subsubsection{Loss Function}

A weighted binary cross entropy will is used as the loss function for optimizing
the wieghts of the model. A normal binary cross entropy function (shown in \autoref{system:eq:bce-loss}) is not
appropriate in this case as the ratio of inactive notes to active notes notes is
on average 10-15 throughout the database.

\begin{equation}
    L^{i} (y_{i}, \hat{y}_{i}) = -(y_{i}\cdot \log(\hat{y}_{i}) + (1-y_{i}) \cdot \log(1-\hat{y}_{i}))
    \label{system:eq:bce-loss}
\end{equation}

This means that the model would overfit to the inactive notes to minimize the loss function which is essentially
the easy way out. A weighted binary cross entropy function accounts for this by
informing the model to fit more for active notes.

\subsubsection{Activation functions}

\ac{ReLU} functions will be used as the
activation function for the hidden layers and a sigmoid activation function will
be used on the output layer as is required in multilabel classification tasks. The sigmoid
activation function is shown in \autoref{system:eq:sigmoid}.

\begin{equation}
    F (x) = \frac{1}{1+ e^{-x}}
    \label{system:eq:sigmoid}
\end{equation}

\subsubsection{Evaluation}
Accuracy is not an appropriate metric in this kind of multilabel classifiction task due to the
imbalance between inactive and active notes. A better measure to use that takes into account both the
precision and recall of the model is the f-measure as shown in \autoref{system:eq:f-score}. The f-measure used in this experiment is based on the
f1-score from the sklearn.metrics libary but is modified so it is appropriate for the
multilabel nature of this problem.

\begin{equation}
    \begin{aligned}
        Precision(P) & = \sum_{i=0}^{N}\frac{TP(i)}{TP(i) + FP(i)}   \\ \\
        Recall(R)    & = \sum_{i=0}^{N}\frac{TP(i)}{TP(i) + FN(i)}   \\ \\
        F~measure    & = 2 \times \sum_{i=0}^{N}\frac{P\cdot R}{P+R} \\
    \end{aligned}
    \label{system:eq:f-score}
\end{equation}