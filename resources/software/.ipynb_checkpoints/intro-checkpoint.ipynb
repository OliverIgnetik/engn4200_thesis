{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial depends on the NumPy, matplotlib, and intervaltree packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np                                       # fast vectors and matrices\n",
    "import matplotlib.pyplot as plt                          # plotting\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from intervaltree import Interval,IntervalTree\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recording of a musical performance is a real-valued time series. The values of this time series represent sound pressure variations sampled at regular intervals, in this case 44,100Hz. The human ear interprets pressure periodicities as musical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = 44100      # samples/second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MusicNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download MusicNet from http://homes.cs.washington.edu/~thickstn/musicnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.load(open('../musicnet.npz','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring MusicNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MusicNet consists of 330 recordings. Each recording is indexed in train_data by a MusicNet id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Number of recordings: ' + str(len(train_data.files))\n",
    "print 'Example MusicNet ids: ' + str(train_data.keys()[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in MusicNet consists of both an audio time series X and a collection of labels Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,Y = train_data['2494'] # data X and labels Y for recording id 2494\n",
    "print type(X)\n",
    "print type(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series X is a floating point array of pressure samples, normalized to the interval [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(2)\n",
    "plt.plot(X[0:30*fs],color=(41/255.,104/255.,168/255.))\n",
    "fig.axes[0].set_xlim([0,30*fs])\n",
    "fig.axes[0].set_xlabel('sample (44,100Hz)')\n",
    "fig.axes[0].set_ylabel('amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play back this time series to hear what it sounds like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Audio(X[0:30*fs],rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels Y are a collection of intervals that indicate the presence of notes in the time series. For convenient access, these notes are stored in an intervaltree. This data structure offers efficient queries for intervals that intersect a particular query point in the time series. For example, we can ask how many notes are being played at time t = 5s in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Notes present at sample ' + str(fs*5) + ' (5 seconds): ' + str(len(Y[fs*5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No notes are being played at time t = 4s (compare this to the plot of the time series X above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Notes present at sample ' + str(fs*4) + ' (4 seconds): ' + str(len(Y[fs*4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each label consists of an interval (start_time,end_time), instrument and note codes, measure and beat, and the note's metrical duration (i.e. note value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(start,end,(instrument,note,measure,beat,note_value)) = sorted(Y[fs*5])[0]\n",
    "print ' -- An example of a MusicNet label -- '\n",
    "print ' Start Time:                          ' + str(start)\n",
    "print ' End Time:                            ' + str(end)\n",
    "print ' Instrument (MIDI instrument code):   ' + str(instrument)\n",
    "print ' Note (MIDI note code):               ' + str(note)\n",
    "print ' Measure:                             ' + str(measure)\n",
    "print ' Beat (0 <= beat < 1):                ' + str(beat)\n",
    "print ' Note Value:                          ' + str(note_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visual Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a visual representation of an aligned score-performance pair by plotting the labels as a function of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stride = 512                         # 512 samples between windows\n",
    "wps = fs/float(stride)               # ~86 windows/second\n",
    "Yvec = np.zeros((int(30*wps),128))   # 128 distinct note labels\n",
    "colors = {41 : .33, 42 : .66, 43 : 1}\n",
    "\n",
    "for window in range(Yvec.shape[0]):\n",
    "    labels = Y[window*stride]\n",
    "    for label in labels:\n",
    "        Yvec[window,label.data[1]] = colors[label.data[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "plt.imshow(Yvec.T,aspect='auto',cmap='ocean_r')\n",
    "plt.gca().invert_yaxis()\n",
    "fig.axes[0].set_xlabel('window')\n",
    "fig.axes[0].set_ylabel('note (MIDI code)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can construct an aural representation of an alignment by mixing a short sine wave into the performance for each note, with the frequency indicated by the score at the time indicated by the alignment. If the alignment is correct, the sine tones will exactly overlay the original performance; if the alignment is incorrect, the mix will sound dissonant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mix_size=4096\n",
    "\n",
    "labels = np.zeros(X.shape)\n",
    "for (onset,offset,label) in sorted(Y):\n",
    "    freq = 440.*2**((label[1] - 69.)/12.)\n",
    "    mark = np.sin(freq*2.*np.pi*np.arange(0,mix_size)/float(fs))\n",
    "    if onset + len(mark) < len(labels):\n",
    "        labels[onset:onset+mix_size] += mark\n",
    "        \n",
    "labels /= np.max(labels)\n",
    "\n",
    "mix = .8*labels + .2*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Audio(mix[0:30*fs],rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
